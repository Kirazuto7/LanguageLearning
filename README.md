# LanguageLearning

An AI-powered language learning application designed to generate dynamic, personalized lessons. This project leverages large language models to create chapters, vocabulary, grammar explanations, and interactive exercises on demand.

## Table of Contents
1. [Preview](#preview)
2. [Key Features](#key-features)
3. [Tech Stack](#tech-stack)
4. [Architecture Overview](#architecture-overview)
5. [Backend Architecture](#backend-architecture)
6. [Frontend Architecture](#frontend-architecture)
7. [Authentication Architecture](#authentication-architecture)
8. [AI Service](#ai-service)
9. [Getting Started](#getting-started)
10. [Running the Application](#running-the-application)
11. [Development Workflow](#development-workflow)
12. [Project Structure](#project-structure)
13. [Attribution](#attribution)

## Preview

![Lesson Book Screenshot](/docs/images/lessonbook-screenshot.png)
![Storybook Screenshot](/docs/images/storybook-screenshot.png)

## Key Features

*   **Dynamic Lesson Generation**: Users can request a new lesson book on a specific topic, language, and difficulty level.
*   **Storybook Generation**: In addition to lessons, users can generate narrative-driven short stories with vocabulary support.
*   **Illustrated Storybooks**: The storybook feature is now fully integrated with an image generation model to create illustrated pages.
*   **AI-Powered Content**: All lesson and story content is generated by an AI model, ensuring unique and varied learning material.
*   **Interactive Proofreading**: Users can submit sentences for practice and receive AI-generated corrections and feedback in real-time. 
*   **Robust AI Interaction**: The backend features a resilient, state-machine-driven engine for AI interaction, which handles JSON schema validation, sanitization, and automatic retries to ensure reliable output.
*   **Content Safety Moderation**: All user-provided topics and text are checked against the OpenAI Moderation API to ensure a safe and responsible platform.
*   **Linguistically-Aware Data Processing**: The backend uses advanced NLP techniques to validate and sanitize AI-generated content, such as correcting Japanese `hiragana` readings before data is saved.
*   **Decoupled & Robust Data Model**: The application's data model separates storage from presentation. Display-specific data like chapter and page numbers are calculated on the client-side, making the backend more resilient to data changes like content deletion or reordering.
*   **Efficient Data Fetching**: For complex, nested data structures, the application uses a modern jOOQ-based strategy to fetch the entire object graph in a single, efficient database query, avoiding common JPA pitfalls like the N+1 problem.
*   **Real-time Progress Updates**: The frontend receives live progress updates via WebSockets for long-running generation jobs.
*   **Multi-Language Support**: The architecture is designed to support multiple languages by configuring different AI models and prompts.
*   **Containerized Development Environment**: The entire application stack (frontend, backend, AI, database) is managed via Docker Compose for easy setup and consistent environments.

## Tech Stack

| Category      | Technology                                                                                             |
|---------------|--------------------------------------------------------------------------------------------------------|
| **Backend**   | Java 21, Spring Boot 3, Spring for GraphQL, Spring Data JPA, **jOOQ**, Spring AI, Project Reactor      |
| **Frontend**  | React, TypeScript, Redux Toolkit (RTK Query), GraphQL, SCSS                                              |
| **AI**        | Ollama (for text), Stable Diffusion (AUTOMATIC1111, for images)                                        |
| **Database**  | PostgreSQL                                                                                             |
| **DevOps**    | Docker & Docker Compose                                                                                |

## Architecture Overview

The application is designed with a clean separation of concerns and is fully containerized. The core services include:
*   `frontend`: The React-based user interface.
*   `backend`: The Spring Boot application providing the core business logic and API.
*   `ai` & `image-api`: Services that host the AI models for text and image generation.
*   `postgres`: The application's database.
*   `init-db`: A short-lived "micro-container" responsible for initializing the database schema on startup. This ensures the `backend` only starts after the database is fully prepared.

## Backend Architecture

The backend is the core of the application, featuring a sophisticated architecture designed for robustness and maintainability. It is built around three custom-designed workflow tools, each chosen for a specific type of problem.

### `SyncWorkflow` (Synchronous Pipeline)
Used for synchronous, transactional setup tasks that must complete atomically. The lessonChapter preparation process is a perfect example, where a sequence of database operations (finding a book, creating a lessonChapter shell) is executed in a clean, declarative pipeline. This ensures data integrity before handing off to a long-running asynchronous job.

### `StateMachine` (Blocking, Asynchronous)
Used for long-running, job-based processes like lessonChapter generation. It orchestrates the sequential creation of lesson components (metadata, vocabulary, grammar, etc.) and sends progress updates to the frontend. It uses a clean `State -> Action` mapping in its configuration, making the workflow easy to read and maintain.

### `ReactiveStateMachine` (Non-Blocking, Asynchronous)
Used internally by the `AIEngine` to handle the complex, branching logic of AI response generation. This state machine also uses a `State -> Action` model and manages the entire lifecycle of an AI call, including:
    1.  **Moderation**: Checking user input for safety before generation.
    2.  **Generation**: Calling the AI model.
    3.  **Validation**: Parsing the response and validating it against a JSON schema.
    4.  **Sanitization**: Attempting to automatically fix common validation errors.
    5.  **Retrying**: Looping back to the generation step with feedback if validation or sanitization fails.

### Data Access Strategy
The backend employs a hybrid data access strategy to leverage the strengths of different tools:
*   **Spring Data JPA**: Used for all simple CRUD (Create, Read, Update, Delete) operations. Its simplicity and convention-over-configuration approach make it ideal for standard entity management.
*   **jOOQ (Java Object Oriented Querying)**: Used for complex, performance-critical read queries. For fetching deeply nested object graphs (like a `StoryBook` with all its stories, pages, and paragraphs), jOOQ's `multiset` operator is used to generate a single, highly efficient SQL query that leverages PostgreSQL's native JSON functions. This avoids the N+1 query problem and the `MultipleBagFetchException` common with complex JPA `JOIN FETCH` operations.

### Other Key Components

*   **`AIEngine`**: A lean, reactive service that acts as the central engine for all AI interactions. It has been refactored to support two distinct pipelines: `generate()` for text-based requests using `AIRequest`, and `generateImages()` for image generation using a separate `AIImageRequest`.
*   **Request Builders**: The `AIRequest` and `AIImageRequest` classes provide a fluent, type-safe builder pattern for constructing AI tasks, ensuring all necessary parameters are provided before execution.

## Frontend Architecture

### Smart Vocabulary Highlighting

The application features a sophisticated vocabulary highlighting system. To provide an accurate and educational experience across multiple languages with complex grammar (e.g., Korean, Spanish, German), the system uses a robust backend-driven approach.

- **Backend NLP Analysis:** The Java backend uses the **Apache Lucene** library to perform linguistic analysis (lemmatization) on both the generated story text and the vocabulary list.
- **Metadata Generation:** The backend identifies the exact conjugated forms of vocabulary words as they appear in the text and saves this list as metadata alongside the paragraph in the database.
- **Efficient Frontend Rendering:** This list of exact words is sent to the frontend, which uses a single, highly efficient regular expression to highlight all occurrences in one pass. This architecture keeps the frontend logic simple and fast while ensuring linguistic accuracy.

### Resilient State & Subscription Management

The application ensures a seamless user experience by making asynchronous tasks, like content generation, persistent and synchronized across multiple browser tabs.

- **Backend Resilience (`ProgressService`):** The backend's progress broadcasting service was refactored to be fully stateless. It correctly handles multiple clients (tabs) subscribing to the same task. When a client disconnects, the data stream remains active for other listeners, preventing the "last tab closed" bug.
- **Frontend Resilience (`graphql-ws`):** The WebSocket client is configured with `retryAttempts`. If a tab is closed, causing the shared connection to drop, the clients in the remaining open tabs will automatically and seamlessly reconnect to the backend, ensuring they continue to receive progress updates without interruption.
- **State Persistence (`redux-persist`):** The Redux state for ongoing tasks is persisted to `localStorage`. When a user opens a new tab or refreshes the page, the state is rehydrated, and the `ProgressSubscriptionManager` automatically re-establishes subscriptions for any tasks that were in progress.
- **Live State Sync (`BroadcastChannel`):** A Redux middleware uses the `BroadcastChannel` API to send notifications between open tabs. When a user logs in, logs out, or changes their settings in one tab, all other tabs are immediately notified and their state is updated to match, ensuring a consistent experience everywhere.

## Authentication Architecture

The application uses a robust, modern token-based authentication system designed for security and a seamless user experience. It supports both traditional username/password registration and Single Sign-On (SSO) via Google, all built around a silent refresh mechanism that prevents the user from being logged out unexpectedly.

*   **Token Strategy**: The system uses two tokens:
    *   A **short-lived access token** (JWT) used to authenticate most API requests. Its short lifetime (e.g., 15 minutes) minimizes the risk of a stolen token.
    *   A **long-lived refresh token** used to obtain a new access token without requiring the user to log in again.

*   **OIDC and Single Sign-On (SSO)**: The application integrates with Google's OpenID Connect (OIDC) provider, allowing users to sign up or log in with a single click. When a new user authenticates via Google, they are guided through a brief onboarding process to set their initial language preferences.

*   **Secure Cookie Storage**: Both tokens are stored in secure, `HttpOnly` cookies, which prevents them from being accessed by client-side JavaScript. This is a critical security measure against XSS attacks. The refresh token also uses token rotation for enhanced security, where a new refresh token is issued each time one is used.

*   **Silent Refresh Mechanism**: The frontend employs a "silent refresh" pattern using custom RTK Query `baseQuery` wrappers.
    1.  When an API request fails with a `401 Unauthorized` or `403 Forbidden` error (indicating an expired access token), the request is not immediately rejected.
    2.  The `baseQuery` wrapper automatically intercepts this error and makes a request to the backend's `/api/users/refresh` endpoint, sending the refresh token.
    3.  If the refresh is successful, the backend responds with a new access token and a new refresh token in `Set-Cookie` headers.
    4.  The wrapper then automatically retries the original, failed API request, which now succeeds with the new access token.

*   **Unified Error Handling**: This silent refresh logic is centralized in custom wrappers for both the REST API (`baseQueryWithReauth`) and the GraphQL API (`graphqlBaseQueryWithReauth`), ensuring a consistent and seamless experience across the entire application.

## AI Service

A dedicated Docker container runs the Ollama service, exposing the language models to the backend. This isolates the heavy AI workload and allows for independent scaling and management (e.g., running on a dedicated GPU instance without affecting the rest of the application).

## Getting Started

### Prerequisites

*   Docker and Docker Compose
*   An NVIDIA GPU with the appropriate drivers (for the GPU profile)
*   A `.env` file configured in the project root.

### Configuration

1.  Create a `.env` file in the root directory of the project. You can copy `.env.example` as a template.
2.  Populate the `.env` file with your desired database credentials. These will be used by both the `postgres` and `backend` services.

```env
# PostgreSQL Database Configuration
POSTGRES_DB=your_db_name
POSTGRES_USER=your_user
POSTGRES_PASSWORD=your_password

# MinIO Object Storage Configuration
MINIO_ROOT_USER=your_minio_user
MINIO_ROOT_PASSWORD=your_minio_password

# AI Service Configuration
# This URL points to the Ollama service. In the Docker Compose setup, \'ai\' is the service name.
SPRING_AI_OLLAMA_BASE_URL=http://ai:11434

# Image Generation API Configuration
# This URL points to the Stable Diffusion (AUTOMATIC1111) API service.
SPRING_AI_IMAGE_BASE_URL=http://image-api:7860
```

## Running the Application

The application can be run using one of two Docker Compose profiles, depending on your hardware.

### With GPU Support (Recommended)

This profile runs the AI service on an NVIDIA GPU for significantly faster performance.

```sh
docker-compose -f docker-compose.gpu.yml up --build
```

### With CPU Only

This profile runs all services on the CPU. AI generation will be noticeably slower.

```sh
docker-compose -f docker-compose.cpu.yml up --build
```

### Accessing the Services

*   **Frontend**: http://localhost:3000
*   **Backend GraphQL Playground**: http://localhost:8080/graphiql
*   **Ollama AI Service**: http://localhost:11434

## Development Workflow

### Resetting the Database

If you need to reset your development database to a clean state at any time, simply run the `init-db` command again. It will drop all tables and recreate them.
```sh
docker-compose run --rm init-db
```

## Project Structure

*   `/`
    *   `backend/`
        *   `src/main/java/com/example/language_learning/`
            *   `ai/` - AI response DTOs and generation logic.
            *   `config/` - Spring, GraphQL, State Machine, and Pipeline configurations.
            *   `lessonbook/` - Feature module for structured lessons.
            *   `security/` - Spring Security configuration, JWT service.
            *   `shared/` - Code shared across multiple features (base entities, shared DTOs, etc.).
            *   `storybook/` - Feature module for narrative stories.
            *   `user/` - User and settings management.
        *   `dockerfiles/` - Dockerfiles for the backend and AI services.
    *   `docs/`
        *   `images/` - Contains diagrams and other visual assets.
    *   `frontend/`
        *   `src/`
            *   `app/` - Redux store setup and core components.
            *   `features/` - Feature-specific components and logic (`lessonBook`, `storybook`, etc.).
            *   `hooks/` - Global, reusable React hooks.
            *   `pages/` - Top-level page components.
            *   `shared/` - Shared code used across the frontend application.
                *   `api/` - RTK Query API slices.
                *   `components/` - Globally shared, simple UI components.
                *   `types/` - Core TypeScript type definitions (`dto.ts`).
                *   `ui/` - More complex, shared UI components.
            *   `widgets/` - Composite components made of smaller features/components.
        *   `.env`: **Note:** If you experience `ENOMEM: not enough memory` errors when running the frontend in Docker, ensure this file exists and contains `WATCHPACK_POLLING_IGNORED=**/node_modules/**` to prevent the file watcher from consuming too much memory.
    *   `docker-compose.gpu.yml` - Docker Compose configuration for GPU.
    *   `docker-compose.cpu.yml` - Docker Compose configuration for CPU.
    *   `.env.example` - Template for environment variables

## Attribution

© 2025 Jordan Sukhnandan All rights reserved.

This project is licensed under the [Apache License 2.0](./LICENSE).  
You are free to use, modify, and distribute the code in accordance with the license terms.  

While redistribution and modification are permitted, please **do not claim this project as your own**.  
If you fork, copy, or build upon this project, proper credit to the original author is appreciated.
